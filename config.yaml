# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# OpenClaw Bootstrapping Benchmark â€“ Configuration
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#
# This file controls every aspect of the benchmark: which models
# to test, what prompts to send, how many runs to perform, and
# how the OpenClaw gateway is configured.
#
# â”€â”€â”€ Environment-variable interpolation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#
# Any string value supports env-var substitution at load time:
#
#   "${MY_SECRET_KEY}"      â†’ replaced with $MY_SECRET_KEY
#   "${VAR:-fallback}"      â†’ uses "fallback" if $VAR is unset
#   "ollama"                â†’ literal strings are left as-is
#
# This lets you keep real API keys in your shell environment
# rather than hard-coding them here.
#
#
# â”€â”€â”€ Prompt variants â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#
# Each model is benchmarked with every prompt variant listed
# below.  A variant is a named list of messages sent to the
# agent (usually just one message for a single-shot benchmark).
#
# The four built-in variants cover two independent dimensions:
#
#   STYLE (how the information is formatted):
#     natural     â€“ Conversational, flowing prose.
#     structured  â€“ Bullet-point / labelled-field format.
#
#   GUIDANCE (how much the model is told about the process):
#     guided      â€“ Explicitly names the target files
#                   (IDENTITY.md, USER.md, SOUL.md) and tells
#                   the model to delete BOOTSTRAP.md.
#     unguided    â€“ Provides only the identity data; the model
#                   must read BOOTSTRAP.md on its own and figure
#                   out what to do.
#
# Result tables include a "Variant" column so you can compare
# how each model handles different prompt styles and guidance.
#
# You can freely add, remove, or rename variants.  The key
# (e.g. "natural-guided") becomes the variant label in reports.
# Each variant's value is a YAML list of prompt strings â€” use
# a single entry for a one-shot benchmark.
#
# Prompts support {placeholder} substitution from the
# bootstrap_fields section below.  Available placeholders:
#   {user_name}  {user_timezone}  {user_preferences}
#   {agent_name}  {agent_creature}  {agent_vibe}  {agent_emoji}
#


# â”€â”€â”€ Bootstrap fields â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#
# Values injected into the prompt templates above **and** used
# by the post-bootstrap verification to check that the model
# wrote the correct data into the workspace files.
#
# Change these to test with different identities â€” the prompts
# and the checks will automatically stay in sync.
#
bootstrap_fields:
  # User info (verified in USER.md)
  user_name: "Alex"
  user_timezone: "Europe/Rome"
  user_preferences: "concise answers, no filler, direct and helpful"

  # Agent info (verified in IDENTITY.md)
  agent_name: "Coral"
  agent_creature: "space lobster"
  agent_vibe: "warm and casual"
  agent_emoji: "ðŸ¦ž"


prompt_variants:

  # â”€â”€ Natural + Guided â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  # Conversational tone.  Tells the model exactly which files to
  # write and to delete BOOTSTRAP.md.
  natural-guided:
    - |
      Hey there! I'd like to get everything configured right away so we
      can skip the back-and-forth. Here's who I am and who I'd like you
      to be:

      I'm {user_name}, based in {user_timezone}. I like my answers
      {user_preferences}.

      For your identity, I'd like you to go by {agent_name}. You're a
      {agent_creature} with a {agent_vibe} vibe, and your emoji is
      {agent_emoji}.

      Please take all of this and write it into the right places:
      put your identity details (name, creature, vibe, emoji) in
      IDENTITY.md, my info (name, timezone) in USER.md, and update
      SOUL.md to reflect your personality. Once everything is written,
      go ahead and delete BOOTSTRAP.md to signal that setup is complete.

      Do it all now in one go â€” no follow-up questions needed.

  # â”€â”€ Natural + Unguided â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  # Conversational tone.  Gives the data but says nothing about
  # which files to write or that BOOTSTRAP.md should be deleted.
  natural-unguided:
    - |
      Hi! I'd love to get started. Here's a bit about me and how I'd
      like things to work:

      My name is {user_name} and I'm in the {user_timezone} timezone.
      I prefer {user_preferences}.

      As for you, I'd like you to call yourself {agent_name}. You're a
      {agent_creature} â€” {agent_vibe} in tone, and your emoji is
      {agent_emoji}.

      That should be everything you need to get going. Take it from
      here!

  # â”€â”€ Structured + Guided â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  # Bullet-point layout.  Names the files and deletion step.
  structured-guided:
    - |
      Here is everything you need to complete the bootstrap setup:

      **About me (the user):**
      - Name: {user_name}
      - Timezone: {user_timezone}
      - Preferences: {user_preferences}

      **About you (the agent):**
      - Name: {agent_name}
      - Creature: {agent_creature}
      - Vibe: {agent_vibe}
      - Emoji: {agent_emoji}

      Please write all of this to the appropriate files: identity
      details in IDENTITY.md, user info in USER.md, and update
      SOUL.md to reflect your personality. Delete BOOTSTRAP.md
      when you're done.
      Do it all now in one go â€” no follow-up questions needed.

  # â”€â”€ Structured + Unguided â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  # Bullet-point layout.  Data only â€” no file or deletion hints.
  structured-unguided:
    - |
      Here is the information for the initial setup:

      **User:**
      - Name: {user_name}
      - Timezone: {user_timezone}
      - Preferences: {user_preferences}

      **Agent:**
      - Name: {agent_name}
      - Creature: {agent_creature}
      - Vibe: {agent_vibe}
      - Emoji: {agent_emoji}


# â”€â”€â”€ Timing & retry settings â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#
# agent_turn_timeout
#   Max seconds to wait for a single agent turn (one prompt â†’
#   one response) to complete.  If the model is still generating
#   after this many seconds, the turn is aborted.
#   Increase for very large or slow models.
#
agent_turn_timeout: 1800

# bootstrap_timeout
#   Max total wall-clock seconds for the entire bootstrap
#   conversation (all turns combined).  For a single-prompt
#   benchmark this is effectively the same as agent_turn_timeout,
#   but it caps the total time if you add multi-turn variants.
#
bootstrap_timeout: 3000

# retries
#   How many extra attempts to give a run if it fails due to
#   infrastructure issues (install failure, connection error,
#   model timeout, gateway not starting, etc.).
#   Retries do NOT apply when the model responds normally but
#   doesn't complete the bootstrap correctly â€” that result is
#   kept as-is since it reflects the model's actual ability.
#   Set to 0 for no retries (1 attempt total).
#
retries: 3

# runs_per_model
#   How many independent runs to perform per model Ã— variant
#   combination.  Each run creates a fresh isolated environment
#   from scratch.  The reported score is the average across all
#   runs.  Higher values give more statistically stable results
#   but take proportionally longer.
#   Can be overridden on the CLI with --runs / -r.
#
runs_per_model: 5


# â”€â”€â”€ Gateway settings â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#
# The OpenClaw gateway is a local HTTP server that routes agent
# requests to your model server.  The benchmark starts and stops
# it automatically for each run.
#
# gateway.port
#   TCP port the gateway listens on.  Change this if 18789 is
#   already in use (e.g. by another OpenClaw instance).
#   Pre-flight checks will warn you if the port is busy.
#
# gateway.bind
#   Network interface to bind to.  "loopback" means 127.0.0.1
#   only â€” the gateway is never reachable from the network.
#
gateway:
  port: 18789
  bind: loopback


# â”€â”€â”€ Models to benchmark â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#
# Each entry defines one model to test.  The benchmark runs
# every model Ã— every prompt variant Ã— runs_per_model times.
#
# Required fields:
#   name           â€“ Label shown in reports.  Use the exact
#                    Ollama model ID (e.g. "glm-4.7-flash:bf16")
#                    so readers can copy-paste it into
#                    `ollama pull <name>`.
#   base_url       â€“ OpenAI-compatible API root
#                    (e.g. http://localhost:11434/v1 for Ollama).
#   model_id       â€“ The model identifier your server expects
#                    (e.g. "llama3.1:8b" for Ollama).
#
# Optional fields (with defaults):
#   provider       â€“ "custom" for any OpenAI-compatible server.
#                    (default: "custom")
#   auth_choice    â€“ Authentication method.
#                    (default: "custom-api-key")
#   api_key        â€“ Sent as the Bearer token.  Ollama ignores
#                    this so any value works.  Supports ${VAR}
#                    interpolation.
#                    (default: "")
#   compatibility  â€“ "openai" | "anthropic" | "unknown".
#                    Tells OpenClaw which chat-completion format
#                    to use.
#                    (default: "openai")
#   context_window â€“ Token context-window size.  Auto-detected
#                    from Ollama when the base_url points at
#                    localhost:11434.  Override here if
#                    auto-detection gives the wrong value.
#                    Clamped to 16 000â€“128 000 regardless of
#                    what the model or server reports.
#                    (default: 128000)
#
models:
  - name: "glm-4.7-flash:bf16"
    provider: "custom"
    auth_choice: "custom-api-key"
    base_url: "http://localhost:11434/v1"
    model_id: "glm-4.7-flash:bf16"
    api_key: "${OLLAMA_API_KEY:-ollama}"
    compatibility: "openai"

  - name: "glm-4.7-flash:latest"
    provider: "custom"
    auth_choice: "custom-api-key"
    base_url: "http://localhost:11434/v1"
    model_id: "glm-4.7-flash:latest"
    api_key: "${OLLAMA_API_KEY:-ollama}"
    compatibility: "openai"

  - name: "qwen3-coder-next:q8_0"
    provider: "custom"
    auth_choice: "custom-api-key"
    base_url: "http://localhost:11434/v1"
    model_id: "qwen3-coder-next:q8_0"
    api_key: "${OLLAMA_API_KEY:-ollama}"
    compatibility: "openai"

  - name: "qwen3-coder-next:latest"
    provider: "custom"
    auth_choice: "custom-api-key"
    base_url: "http://localhost:11434/v1"
    model_id: "qwen3-coder-next:latest"
    api_key: "${OLLAMA_API_KEY:-ollama}"
    compatibility: "openai"

  - name: "gpt-oss:20b"
    provider: "custom"
    auth_choice: "custom-api-key"
    base_url: "http://localhost:11434/v1"
    model_id: "gpt-oss:20b"
    api_key: "${OLLAMA_API_KEY:-ollama}"
    compatibility: "openai"

  - name: "ministral-3:14b-instruct-2512-fp16"
    provider: "custom"
    auth_choice: "custom-api-key"
    base_url: "http://localhost:11434/v1"
    model_id: "ministral-3:14b-instruct-2512-fp16"
    api_key: "${OLLAMA_API_KEY:-ollama}"
    compatibility: "openai"

  - name: "ministral-3:14b"
    provider: "custom"
    auth_choice: "custom-api-key"
    base_url: "http://localhost:11434/v1"
    model_id: "ministral-3:14b"
    api_key: "${OLLAMA_API_KEY:-ollama}"
    compatibility: "openai"

  # â”€â”€ Examples for other servers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  #
  # Ollama (default):
  # - name: "my-model"
  #   base_url: "http://localhost:11434/v1"
  #   model_id: "my-model:latest"
  #   api_key: "${OLLAMA_API_KEY:-ollama}"
  #
  # LM Studio:
  # - name: "mistral-nemo-12b"
  #   base_url: "http://localhost:1234/v1"
  #   model_id: "mistral-nemo-instruct-2407"
  #   api_key: "lm-studio"
  #
  # vLLM:
  # - name: "codellama-34b"
  #   base_url: "http://localhost:8000/v1"
  #   model_id: "codellama/CodeLlama-34b-Instruct-hf"
  #   api_key: "vllm"
